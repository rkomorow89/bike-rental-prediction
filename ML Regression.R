
#change to your own source file location
setwd("~/Daten-Orchestrierung/Auftrag Caner")

# The Data Set (source: https://www.kaggle.com/datasets/aguado/bike-rental-data-set-uci?resource=download)
#The existing bicycle rental systems in large cities have a system automated collection and return of the 
#vehicle through a network of stations distributed throughout the entire metropolis. With the use of these 
#systems, people can rent a bike in a location and return it in a different one depending on your needs. 
#The data generated by these systems are attractive to researchers due to variables such as the duration of
#the trip, departure and destination points and travel time. Therefore, exchange systems Bicycles work as a
#network of sensors that are useful for mobility studies. With In order to improve management, one of these 
#companies needs to anticipate the demand that there will be in a certain range of time depending on factors 
#such as the time zone, the type of day (weekday or holiday), the weather, etc.
#The objective of this data set is to predict the demand in a series of specific time slots, using the
#historical data set as the basis to build a linear model.
#Kaggle delivers two data sets containing the number of rented bicycles in different time slots:
#1. Training data. They will contain the response variable (number of bicycles rented in that strip)
#2. Test data. They will not contain the response variable and the response variable must be predicted based 
#on the historical data of the training set.

# load the train dataset
# number of rows: 7689, number of columns: 12
# we use only the data from train.csv as this is the only file that contains data for the target variable
# from this dataset, we only use the first 1500 rows so that model training is sped up

bikes_df <- read.table("train.csv",sep=";",skip=1)[0:1500,]

#description of the variables with their data types:
#id: time slot identifier (not related to time order) – integer / discrete
#year: year (2011 or 2012) ) – integer / discrete
#hour: hour of the day (0 to 23) – integer / discrete
#season: 1 = winter, 2 = spring, 3 = summer, 4 = autumn – integer / discrete
#holiday: if the day was a holiday ) – integer / discrete
#workingday: if the day was a working day (neither a holiday nor a weekend) – integer / discrete
#weather: four categories (1 to 4) ranging from best to worst weather ) – integer / discrete
#temp: temperature in degrees Celsius – numeric / continuous
#atemp: sensation of temperature in degrees Celsius – numeric / continuous
#humidity: relative humidity – integer / discrete
#windspeed: wind speed (km/h) – numeric / continuous
#target: total number of rentals in that time slot – integer / discrete

#change column names
colnames(bikes_df) <- c("id","year","hour","season","holiday","workingday","weather","temp","atemp","humidity","windspeed","target")

#show structure of dataset
str(bikes_df)

#Check for multicollinearity in the data via Variance Inflation Factors (VIFs)

# Load the car package
library(car)

# Fit a multiple linear regression model
lm_model <- lm(target ~ ., data = bikes_df)

# Calculate the VIF for each predictor variable
vif_values <- vif(lm_model)

# Identify predictor variables with VIF > 10
high_vif <- names(vif_values)[vif_values > 10]
print(high_vif)

#The VIF for columns temp and atemp are bigger than 10, which is a sign for multicollinearity, so one of them should be removed

# Calculate the correlation matrix
cor_matrix <- cor(bikes_df)

# View the correlation matrix
cor_matrix

# Load the corrplot package
install.packages("corrplot")
library(corrplot)

# Visualize the correlation matrix as a heatmap
corrplot(cor_matrix, method = "color")

#the id column has low correlations with other features, so it should be left out

# Load the rlang and dplyr packages
remove.packages("rlang")
install.packages("rlang")
library(rlang)
library(dplyr)

# Exclude columns "id" and "temp"
bikes_df <- select(bikes_df, -id, -temp)

#show structure of dataset
str(bikes_df)

# Set the seed for reproducibility
set.seed(123)

#The data is divided into a training set (60%), a validation set (20%) and a test set (20%). Cross-validation 
#is not used.

# Shuffle the rows of the dataframe
bikes_df <- bikes_df[sample(nrow(bikes_df)), ]

# Split the data into three sets (number of rows: 900/300/300)
train_index <- 1:round(0.6 * nrow(bikes_df))
validation_index <- (train_index[length(train_index)]+1):(train_index[length(train_index)]+round(0.2 * nrow(bikes_df)))
test_index <- (validation_index[length(validation_index)]+1):nrow(bikes_df)

train_set <- bikes_df[train_index, ]
validation_set <- bikes_df[validation_index, ]  
test_set <- bikes_df[test_index, ]

#load package for normalization
install.packages("scales")
library("scales")

#As the provided dataset is not yet normalized, this has to be done here so that the models deliver
#more appropriate predictions

# Normalize the training, validation and test data
train_set_normalized <- sapply(train_set, rescale)
validation_set_normalized <- sapply(validation_set, rescale)
test_set_normalized <- sapply(test_set, rescale)

# select the columns to use as predictors and the column to use as the target
#X_train_normalized <- train_set_normalized[, c("year","hour","season","holiday","workingday","weather","atemp","humidity","windspeed")]
X_train_normalized <- subset(train_set_normalized, select = -target)
Y_train_normalized <- train_set_normalized[, "target"]

#X_val_normalized <- validation_set_normalized[, c("year","hour","season","holiday","workingday","weather","atemp","humidity","windspeed")]
X_val_normalized <- subset(validation_set_normalized, select = -target)
Y_val_normalized <- validation_set_normalized[, "target"]

#X_test_normalized <- test_set_normalized[, c("year","hour","season","holiday","workingday","weather","atemp","humidity","windspeed")]
X_test_normalized <- subset(test_set_normalized, select = -target)
Y_test_normalized <- test_set_normalized[, "target"]

#Method: we treat the problem of the prediction of bike rentals as a regression problem, as we want to predict 
#a specific number that can be different with each new collection of data.
#to analyse the data, we use two algorithms: Support Vector Regression (SVR) and Artificial Neural Networks (NN)

###Support Vector Regression

#The first algorithm we try is SVR. This is a type of Support Vector Machine (known for classification problems)
#that is used for regression problems. SVR gives us the flexibility to define how much error is acceptable 
#in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data.
#SVR models contain several hyperparameters, the most important ones are:
#kernel: its main function is to take low dimensional input space and transform it into a higher-dimensional 
#space. It is mostly useful in non-linear separation problem. Examples: linear, radial, polynomial, sigmoid
#cost: C is the penalty parameter, which represents misclassification or error term. The misclassification 
#or error term tells the SVM optimisation how much error is bearable. This is how you can control the trade-off
#between decision boundary and misclassification term. when C is high it will classify all the data points 
#correctly, also there is a chance to overfit.
#gamma: It defines how far data points influence the calculation of a plausible line of separation. when gamma 
#is higher, nearby points will have high influence; low gamma means far away points are also considered to get
#the decision boundary.

# install and load the e1071 package
install.packages("e1071")
library(e1071)

# train the SVR model on training data
svr_model <- svm(X_train_normalized, Y_train_normalized)
#svr_model <- svm(target ~ ., data = train_set_normalized)

# make predictions on the validation set
val_predictions_svr <- predict(svr_model, X_val_normalized)

#show hyperparameters of SVR model
summary(svr_model)

#to measure how well the model predicts the number of bike rentals, the metrics R², MSE and RMSE are calculated
#R² is a comparison of the sum of squared residuals (SSR) with the total sum of squares(TSS). 
#SSR is calculated by the summation of squares of perpendicular distance between data points and the best-fitted line.
#TSS is calculated by summation of squares of perpendicular distance between data points and the average line.
#R² is calculated by dividing SSR through TSS and subtracting the result from 1
#The more the value of R² is near 1, the better is the model
#to calculate the MSE (Mean Squared Error), you take the difference between a model’s predictions and the
#ground truth (i.e. actual values), square it, and average it out across the whole dataset.The MSE will never
#be negative, since we are always squaring the errors. The closer the value of MSE is near 0, the better is 
#the model. By calculating the square root of MSE, you get the RMSE (Root mean square error).
 
# Calculate the sum of squared residuals
ssr_svr <- sum((val_predictions_svr - Y_val_normalized) ^ 2) 

# Calculate the total sum of squares
tss_svr <- sum((Y_val_normalized - mean(Y_val_normalized)) ^ 2) 

# Calculate the R-squared value
r_squared_svr <- 1 - ssr_svr/tss_svr
print(r_squared_svr)

# Calculate the squared differences between the predictions and the actual values
squared_differences_svr <- (val_predictions_svr - Y_val_normalized)^2

# Calculate the MSE
mse_svr <- mean(squared_differences_svr)
print(mse_svr)

# Calculate the RMSE
rmse_svr <- sqrt(mse_svr)
print(rmse_svr)

# Create a data frame with the predictions and actual values
prediction_data_svr <- data.frame(predictions = val_predictions_svr, actual = Y_val_normalized)

# load packages for visualizations
remove.packages("rlang")
install.packages("rlang")
library(rlang)
install.packages("ggplot2")
library(ggplot2)

# Create the scatterplot (x axis: actual values, y axis: predicted values) and draw a diagonal line in it
ggplot(prediction_data_svr, aes(x = val_predictions_svr, y = Y_val_normalized, color = "blue")) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  guides(col = FALSE) +
  xlim(-0.1,1) +
  ylim(-0.1,1)

#To optimize the prediction quality of the SVR Model, its hyperparameters are tuned via a grid search.
#For this, you create a dictionary of possible values for different hyperparameters and then train models for
#every combination of values. A grid search will find out the best combination of hyperparameters that will then
#be used to train the model with validation data anew. R² / MSE / RSME will be calculated again to compare
#how much the model performance has been improved.

# Tune the hyperparameters of the SVM model using training data
tune_result_svr <- tune.svm(X_train_normalized, Y_train_normalized, cost = 10^(-1:2), gamma=seq(0,0.5,by=0.1), epsilon=seq(0,0.5,by=0.1))

summary(tune_result_svr)

#list optimal values
best_cost <- tune_result_svr$best.parameters$cost
print(best_cost)
best_gamma <- tune_result_svr$best.parameters$gamma
print(best_gamma)
best_epsilon <- tune_result_svr$best.parameters$epsilon
print(best_epsilon)

# Train the SVM model using the optimal hyperparameters
tuned_svr_model <- tune_result_svr$best.model

# make predictions on the validation set with the tuned model
val_predictions_tuned_svr <- predict(tuned_svr_model, X_val_normalized)

#show hyperparameters of SVR model
summary(tuned_svr_model)

# Calculate the sum of squared residuals
ssr_tuned_svr <- sum((val_predictions_tuned_svr - Y_val_normalized) ^ 2) 

# Calculate the total sum of squares
tss_tuned_svr <- sum((Y_val_normalized - mean(Y_val_normalized)) ^ 2) 

# Calculate the R-squared value
r_squared_tuned_svr <- 1 - ssr_tuned_svr/tss_tuned_svr
print(r_squared_tuned_svr)

# Calculate the squared differences between the predictions and the actual values
squared_differences_tuned_svr <- (val_predictions_tuned_svr - Y_val_normalized)^2

# Calculate the MSE
mse_tuned_svr <- mean(squared_differences_tuned_svr)
print(mse_tuned_svr)

# Calculate the RMSE
rmse_tuned_svr <- sqrt(mse_tuned_svr)
print(rmse_tuned_svr)

# Create a data frame with the predictions and actual values
prediction_data_svr <- data.frame(predictions = val_predictions_tuned_svr, actual = Y_val_normalized)

# Create the scatterplot (x axis: actual values, y axis: predicted values) and draw a diagonal line in it
ggplot(prediction_data_svr, aes(x = val_predictions_tuned_svr, y = Y_val_normalized, color = "green")) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  guides(col = FALSE) +
  xlim(-0.1,1) +
  ylim(-0.1,1)

#With this best model, predictions for the test data will be made and then visualized in a scatterplot 
#together with the actual values. The nearer the points are to the diagonal line (where predictions = actual),
#the better is the model. This plot will be used later to compare the results of the NN model with the SVR model.

# make predictions on the test set
test_predictions_svr <- predict(tuned_svr_model, X_test)

# Create a data frame with the predictions and actual values
prediction_data_svr <- data.frame(predictions = test_predictions_svr, actual = Y_test)

# Create the scatterplot (x axis: actual values, y axis: predicted values) and draw a diagonal line in it
ggplot(prediction_data_svr, aes(x = test_predictions_svr, y = Y_test, color = "red")) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  guides(col = FALSE) + 
  xlim(-0.1,1) +
  ylim(-0.1,1)

###Artificial Neural Network

#For the second algorithm we'll be using Artificial Neural Networks as they can be used for regression problems 
#as well. NNs are modeled loosely on the human brain and can consist of thousands or even millions of simple
#processing nodes that are densely interconnected. They are organized into layers of nodes, and they’re
#“feed-forward”: The data moves through them in only one direction.
#NNs consist of node layers, containing an input layer, one or more hidden layers, and an output layer. 
#Each node, or artificial neuron, connects to another and has an associated weight and threshold.
#So each one is composed of input data, weights, a bias (or threshold) and an output. The weights help determine 
#the importance of any given variable: Larger ones will contribute significantly more to the output compared to 
#other inputs. To fit the data to the model, the loss function is minimized, which for NN-regression is the 
#mean of squared errors MSE over the training set. The model adjusts its weights and biases to reach the point
#of convergence, or the local minimum.
#In the case of fitting a neural network, it is much faster if the data are scaled/normalised first and also
#gives more appropriate results if the features that are used have different scales and ranges.

# Load the neuralnet package
install.packages("neuralnet")
library(neuralnet)

# Define the neural network architecture. 
#Feature to be predicted: target, 
#features for prediction: year,hour,season,holiday,workingday,weather,atemp,humidity,windspeed
#data for training: normalized train data (values between 0 and 1)
#the following hyperparameters are used:
#hidden: the number of hidden layers and the number of units in each hidden layer. These parameters determine 
#the complexity of the network and its capacity to model the underlying relationships in the data. In this model,
#two hidden layers with two nodes in the first layer and three nodes in the second are chosen so that the 
#complexity of the model is not too high (see plot of the network after training)
#learningrate: this parameter determines the step size used by the optimization algorithm when updating the 
#weights and biases during training. A larger learning rate can lead to faster convergence, but can also make
#the optimization process more unstable. In this model, a low learning rate is chosen so that the model is more 
#stable and less prone to oscillation or divergence.
#linear.output: is TRUE for continuous response, FALSE for categorical response. As the model is used for 
#regression, we set this parameter to TRUE.

nn_model <- neuralnet(formula = target ~ .,
                      data = train_set_normalized,
                      hidden = c(4, 4),
                      learningrate = 0.0001,
                      linear.output = FALSE,
                      rep = 5,
                      stepmax = 1e5)

#get column number of epoch with minimum error
result.matrix = nn_model$result.matrix
error = head(result.matrix, n=1)
print(error)
min_col <- apply(error, 1, which.min)
print(min_col)

# Get the summary of the model
summary(nn_model)

# Make predictions on the validation data using best epoch
val_predictions_nn <- compute(nn_model, X_val_normalized, rep=min_col)$net.result

results <- data.frame(actual = Y_val_normalized, prediction = val_predictions_nn)

#convert predicted and actual values back to original format, then compute deviations and average them
predicted=results$prediction * abs(diff(range(validation_set$target))) + min(validation_set$target)
actual=results$actual * abs(diff(range(validation_set$target))) + min(validation_set$target)
comparison=data.frame(predicted,actual)
deviation=((actual-predicted)/actual)
comparison=data.frame(predicted,actual,deviation)
mean_deviation=1-abs(mean(deviation))
print(mean_deviation)

# Calculate the sum of squared residuals
ssr_nn <- sum((val_predictions_nn - Y_val_normalized) ^ 2) 

# Calculate the total sum of squares
tss_nn <- sum((Y_val_normalized - mean(Y_val_normalized)) ^ 2) 

# Calculate the R-squared value
r_squared_nn <- 1 - ssr_nn/tss_nn
print(r_squared_nn)

# Calculate the squared differences between the predictions and the actual values
squared_differences_nn <- (val_predictions_nn - Y_val_normalized)^2

# Calculate the MSE
mse_nn <- mean(squared_differences_nn)

# Calculate the RMSE
rmse_nn <- sqrt(mse_nn)
print(rmse_nn)

# Make predictions on the test set
test_predictions_nn <- compute(nn_model, X_test_normalized)$net.result
test_predictions_nn <- compute(nn_model, X_test)$net.result

# Create a data frame with the predictions and actual values
prediction_data_nn <- data.frame(predictions = test_predictions_nn, actual = Y_test_normalized)
prediction_data_nn <- data.frame(predictions = test_predictions_nn, actual = Y_test)

# Create the scatterplot
ggplot(prediction_data_nn, aes(x = test_predictions_nn, y = Y_test_normalized)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(-0.1,1) +
  ylim(-0.1,1)

# Create the scatterplot
ggplot(prediction_data_nn, aes(x = test_predictions_nn, y = Y_test)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(-0.1,1) +
  ylim(-0.1,1)

# Discussion
#If we look at the evaluation metrics, we can see that the value for R² of the NN model is nearer to 1
#compared to the SVR model, even after its hyperparameter optimization. For this reason, a grid search was not
#necessary for the NN model.
#if you compare both scatterplots of predicted vs. actual values, one can see that the NN predictions 
#are closer to the diagonal line compared to the SVR predictions, which confirms the superiority of the NN model.
